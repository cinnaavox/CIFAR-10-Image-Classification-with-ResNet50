# ğŸŒŸ CIFAR-10 Bildklassifikation mit ResNet50  
**Deep Learning Â· Computer Vision Â· Transfer Learning Â· TensorFlow**

---

## ğŸ§­ Ãœberblick

Dieses Repository enthÃ¤lt zwei vollstÃ¤ndig ausgearbeitete Google-Colab-Notebooks, in denen ich den CIFAR-10-Datensatz mithilfe von **Transfer Learning** und **ResNet50** klassifiziere.

Das Projekt war Teil meiner Data-Analytics-Fortbildung und mein Ziel war es, einen **kompletten Computer-Vision-Workflow selbststÃ¤ndig** aufzubauen â€“ ohne Schritt-fÃ¼r-Schritt-Anleitung.

Um das Verhalten des Modells besser zu verstehen, habe ich **zwei Varianten** erstellt:

### ğŸ”¹ Experiment A â€“ 10.000 Trainingsbilder (Aufgabenstellung)  
Version, die ich prÃ¤sentiere.  
Zeigt Overfitting sehr klar.

### ğŸ”¹ Experiment B â€“ 50.000 Trainingsbilder (voller Datensatz)  
Ein Erweiterungsexperiment, das ich danach erstellt habe.  
Zeigt deutlich stabileres Lernen und bessere Generalisierung.

Beide Versionen liegen diesem Repository bei.

---

### ğŸ“‚ Repository-Struktur

cifar10-resnet50/

- CV_Project_Final_Version.ipynb        -> Version mit 10.000 Bildern (PrÃ¤sentation)
- CV_Project_Version_2_all_data.ipynb   -> Version mit allen 50.000 Trainingsbildern
- README.md

---

## ğŸ¯ Projektziele

- CIFAR-10-Datensatz verstehen  
- VollstÃ¤ndige Bildklassifikations-Pipeline aufbauen  
- ResNet50 als Feature-Extractor nutzen  
- Eigenen Klassifikationskopf ergÃ¤nzen  
- Zwei Trainingsphasen durchfÃ¼hren:
  - **Head-Only Training**
  - **Fine-Tuning**
- Generalisierung vs. Overfitting analysieren  
- Ergebnisse durch Plots und Beispiele visualisieren  
- Unterschiede zwischen kleinem und groÃŸem Datensatz verstehen  

---

# ğŸ“¦ Der CIFAR-10 Datensatz

- 60.000 RGB-Bilder  
- 10 Klassen (z. B. airplane, dog, car, ship â€¦)  
- AuflÃ¶sung: **32 Ã— 32 Pixel**  
- 50.000 Trainingsbilder  
- 10.000 Testbilder  

Die geringe AuflÃ¶sung macht CIFAR-10 anspruchsvoll â€“ perfekt, um zu beobachten, wie gut ein starkes, vortrainiertes Modell damit umgehen kann.

---

# ğŸ§ª Experiment A â€“ 10.000 Bilder (Aufgabe)

### **Workflow**
- Begrenzen auf 10k Trainingsbilder  
- Normalisierung (0â€“1)  
- One-Hot-Encoding der Labels  
- Laden von ResNet50 ohne Top-Layer  
- Eigenen Klassifikationskopf bauen:
  - GAP â†’ Dense(256) â†’ Dense(64) â†’ Dense(10)  
- Phase 1 â†’ nur Kopf trainieren  
- Phase 2 â†’ ResNet50 auftauen und feinabstimmen  

---

### **Ergebnisse**

#### **Head-Training**
- Trainingsgenauigkeit: ~0,37  
- Testgenauigkeit: ~0,36  
- Stabile, parallele Kurven  
- Modell lernt etwas, aber bleibt limitiert  

#### **Fine-Tuning**
- Trainingsgenauigkeit: **bis 97 %**  
- Testgenauigkeit: **nur ~26,7 %**  
- Validation-Loss extrem hoch  
- â†’ **klassisches Overfitting**

---

## ğŸ§  Learnings aus Experiment A

- 10k Bilder sind zu wenig fÃ¼r ein groÃŸes Modell wie ResNet50  
- Fine-Tuning ist extrem empfindlich gegenÃ¼ber Datenmenge  
- Overfitting ist sowohl in Zahlen als auch in Plots sofort sichtbar  
- Vorhersagen bestÃ¤tigen das Muster: manche Treffer, viele Fehler  

Diese Version eignet sich perfekt, um **Generalisation vs. Memorisation** zu demonstrieren.

---

# ğŸ§ª Experiment B â€“ 50.000 Bilder (voller Datensatz)

Nachdem Experiment A abgeschlossen war, habe ich das Notebook neu aufgebaut â€“ dieses Mal mit **allen 50.000 Trainingsbildern**.

### **Workflow**
Gleiche Architektur, gleiche Hyperparameter.  
Nur die Datenmenge wurde erhÃ¶ht.

---

### **Ergebnisse**
- **Testaccuracy: 66,30 %**  
- deutlich stabilere Trainingskurven  
- viel weniger Overfitting  
- bessere, nachvollziehbarere Vorhersagen  

---

## ğŸ§  Learnings aus Experiment B

- Datenmenge ist einer der stÃ¤rksten Einflussfaktoren im Deep Learning  
- Fine-Tuning funktioniert deutlich besser mit mehr Daten  
- ResNet50 wird stabiler, je mehr Beispiele es sieht  
- Auch mit 32px-Bildern ist gute Performance mÃ¶glich  
- Der Unterschied zwischen 10k und 50k ist **dramatisch â€“ und extrem lehrreich**  

---

# ğŸ” Vergleich: 10k vs. 50k

| Faktor | 10.000 Bilder | 50.000 Bilder |
|--------|----------------|----------------|
| Datenmenge | reduziert | vollstÃ¤ndig |
| Trainingsverhalten | instabil | harmonisch |
| Testaccuracy | ~26,7 % | ~66,3 % |
| Overfitting | stark | deutlich geringer |
| Vorhersagen | wechselhaft | deutlich zuverlÃ¤ssiger |

ğŸ‘‰ Der Vergleich zeigt sehr klar, warum **DatenquantitÃ¤t** bei Deep Learning entscheidend ist.

---

# ğŸ”§ Technischer Workflow (beide Versionen)

1. CIFAR-10 laden  
2. Normalisieren  
3. Labels One-Hot-Encoden  
4. ResNet50 laden (ohne Top-Layer)  
5. Klassifikationskopf bauen  
6. Head-Training  
7. Fine-Tuning  
8. Test-Evaluation  
9. Lernkurven visualisieren  
10. Vorhersagen anzeigen  
11. Zweites Experiment mit 50k Bildern durchfÃ¼hren  

---

# ğŸ’¡ Was ich aus dem Projekt gelernt habe

- Wie Transfer Learning praktisch funktioniert  
- Warum das Einfrieren des Basismodells wichtig ist  
- Wie stark die Datenmenge die Generalisierung beeinflusst  
- Wie man Overfitting erkennt und interpretiert  
- Wie CNNs visuelle Muster verarbeiten  
- Warum Trainingsaccuracy allein nicht aussagekrÃ¤ftig ist  
- Wie Architektur, Lernrate und Datenmenge zusammen wirken  

Dieses Projekt hat mein VerstÃ¤ndnis fÃ¼r Deep Learning und Computer Vision massiv vertieft.

---

# ğŸš€ NÃ¤chste Schritte

- Data Augmentation  
- L2-Regularisierung & Dropout  
- Teilweises Auftauen einzelner ResNet-Schichten  
- Alternatives Modell (EfficientNet, MobileNet) testen  
- LÃ¤ngeres Fine-Tuning auf GPU/TPU  
- Mixed Precision Training  

---

ğŸ“„ Notebook (10.000 Bilder â€“ PrÃ¤sentationsversion):
ğŸ‘‰ [Hier klicken](https://colab.research.google.com/drive/1LtZH3GiPX27fzAkld4mNZo2LKuOTeZGW?usp=sharing)

ğŸ“„ Notebook (50.000 Bilder â€“ vollstÃ¤ndiger Datensatz):
ğŸ‘‰ [Hier klicken](https://colab.research.google.com/drive/1FQVA2m7Zo43ApRV2ry36I7CTq9AWDsnS?usp=sharing)

---

# ğŸ“« Kontakt

Wenn du Feedback hast oder Ã¼ber Deep Learning sprechen mÃ¶chtest â€“ jederzeit gerne!
